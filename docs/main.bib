@article{vaswani2017attention,
  title   = {Attention is all you need},
  author  = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal = {Advances in neural information processing systems},
  volume  = {30},
  year    = {2017}
}

@article{luong2015effective,
  title   = {Effective approaches to attention-based neural machine translation},
  author  = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D},
  journal = {arXiv preprint arXiv:1508.04025},
  year    = {2015}
}

@article{devlin2018bert,
  title   = {Bert: Pre-training of deep bidirectional transformers for language understanding},
  author  = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal = {arXiv preprint arXiv:1810.04805},
  year    = {2018}
}

@misc{website-mlm,
  title        = {{Machine Learning Mastery} The Transformer Model},
  author       = {Machine Learning Mastery},
  howpublished = {\url{https://machinelearningmastery.com/the-transformer-model/}},
  note         = {Accessed: 2022-07-04}
}
